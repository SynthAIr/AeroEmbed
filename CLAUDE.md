# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## AeroEmbed Project Overview

AeroEmbed is a specialized framework for analyzing flight operational data through embeddings generated by TabSyn (VAE + Diffusion model). The project transforms mixed categorical/numerical flight data into meaningful vector representations for operational insights in Air Traffic Management.

## Core Architecture

### TabSyn Two-Stage Architecture
The system combines **VAE (Variational Autoencoder)** with **Diffusion Models**:
- **VAE Stage**: Learns compact representations of flight data patterns via transformer-based encoder/decoder
- **Diffusion Stage**: Models distribution in latent space for high-quality synthetic data generation
- **Embedding Extraction**: Uses trained encoder to generate analysis-ready embeddings

### Data Flow Patterns
1. **Training Phase**: `prepare_data.py` → VAE training → Diffusion training → Model artifacts
2. **Embedding Phase**: New data → Trained encoder → Embeddings (numpy arrays)  
3. **Analysis Phase**: Embeddings + Original data → Domain-specific analysis → Visualizations

### Dual Preprocessing Views
The preprocessing creates two data formats:
- **Generative View** (`preprocess_flight_data()`): Optimized for autoregressive generation
- **Predictive View** (`preprocess_flight_data_for_prediction()`): Optimized for analysis tasks

## Essential Commands

### Development Setup
```bash
# Install dependencies
poetry install
poetry shell

# Verify installation
python -c "import aeroembed; print('Installation successful!')"
```

### Complete Training Workflow
```bash
# 1. Prepare data
python scripts/embedding/prepare_data.py --input_path data/flights.csv --output_train_path data/real/train.csv --output_test_path data/real/test.csv

# 2. Train TabSyn model
python scripts/embedding/train_tabsyn_model.py --train_path data/real/train.csv --model_dir models/flight_model --vae_epochs 200 --diffusion_epochs 1000 --embedding_dim 4

# 3. Extract embeddings from test data
python scripts/embedding/extract_embeddings.py --data_path data/real/test.csv --model_dir models/flight_model
```

### Analysis Commands (using pre-trained model)
```bash
# Airport network analysis
python scripts/analysis/operational_patterns/analyze_airport_network.py --embeddings models/tabsyn/ckpt/train_z.npy --data data/real/train.csv --threshold 0.9

# Carrier operations comparison  
python scripts/analysis/operational_patterns/analyze_carrier_operations.py --embeddings models/tabsyn/ckpt/train_z.npy --data data/real/train.csv --sample-size 200000

# Seasonal patterns analysis
python scripts/analysis/operational_patterns/analyze_seasonal_patterns.py --embeddings models/tabsyn/ckpt/train_z.npy --data data/real/train.csv

# Anomaly detection
python scripts/analysis/clustering/detect_anomalies.py --embeddings models/tabsyn/ckpt/train_z.npy --data data/real/train.csv --contamination 0.05

# Embedding space visualization
python scripts/visualization/navigate_embedding_space.py --embeddings models/tabsyn/ckpt/train_z.npy --data data/real/train.csv
```

### Generate Synthetic Data
```bash
python scripts/embedding/generate_synthetic_flights.py --model_dir models/tabsyn --num_samples 10000 --output_path synthetic/flights_10k.csv
```

## Working with Pre-trained Models

The repository includes a pre-trained TabSyn model in `models/tabsyn/` with:
- **Embeddings**: 4D latent space representations
- **Training Data**: Pre-processed flight embeddings in `models/tabsyn/ckpt/train_z.npy`
- **Configuration**: Model architecture and preprocessing parameters

To use existing embeddings for analysis:
```bash
# Most analysis scripts work directly with the pre-trained embeddings
python scripts/analysis/operational_patterns/analyze_*.py --embeddings models/tabsyn/ckpt/train_z.npy --data [your_flight_data.csv]
```

## Key File Relationships

### Model Training Dependencies
- `src/aeroembed/generators/tabsyn/tabsyn.py`: Main orchestrator class
- `src/aeroembed/generators/tabsyn/vae/model.py`: VAE implementation with transformer encoder/decoder
- `src/aeroembed/generators/tabsyn/model.py`: Diffusion model components
- `src/aeroembed/preprocessing/flight_data.py`: Flight-specific data transformations

### Analysis Script Architecture
All analysis scripts follow the pattern: 
```python
embeddings, metadata = load_data(embeddings_path, data_path)
# Domain-specific analysis using embeddings + original data
# Output: PNG visualizations + CSV results
```

### Critical Configuration Files
- `models/tabsyn/data/info.json`: Column mappings and data types
- `models/tabsyn/training_params.json`: Model architecture parameters  
- `models/tabsyn/ckpt/config.pkl`: Tokenizer configuration for consistent preprocessing

## Data Requirements

### Input Flight Data Format
Required columns for flight data:
- `IATA_CARRIER_CODE`, `DEPARTURE_IATA_AIRPORT_CODE`, `ARRIVAL_IATA_AIRPORT_CODE`
- `AIRCRAFT_TYPE_IATA`, `SCHEDULED_DEPARTURE_UTC`, `SCHEDULED_ARRIVAL_UTC`
- `DEPARTURE_ACTUAL_OUTGATE_UTC`, `ARRIVAL_ACTUAL_INGATE_UTC`

The preprocessing automatically generates temporal features (delays, durations, time components).

### Analysis Data Alignment
For analysis scripts to work correctly:
1. Use the same preprocessing pipeline as training data
2. Ensure embedding count matches data row count
3. Include all required columns for the specific analysis

## Common Workflows

### Adding New Analysis
1. Follow existing analysis script patterns in `scripts/analysis/`
2. Load embeddings + original data using established utilities
3. Apply domain-specific analysis (clustering, similarity, temporal patterns)
4. Generate PNG visualizations and CSV outputs

### Working with New Flight Data
1. Ensure data matches required column format
2. Use `extract_embeddings.py` with existing trained model
3. Run analysis scripts with new embeddings + original data

### Model Retraining
Only needed when:
- Working with fundamentally different data domains
- Requiring different embedding dimensions
- Significant changes to feature engineering

The pre-trained model works well for standard flight operational analysis tasks.

## Output Patterns

All analysis scripts automatically create:
- High-resolution PNG visualizations (300 DPI)
- CSV files with quantitative results
- Organized output directories with descriptive filenames

Results are saved to `results/` directory with script-specific subdirectories.

## Memory and Performance

- Use `--sample-size` parameter for large datasets to prevent memory issues
- GPU usage: specify `--device cuda` for training and embedding extraction
- Batch processing: Most scripts handle large datasets through automatic batching
- Default sample sizes are optimized for typical analysis tasks while maintaining statistical validity

## Memories

### Environment Management
- Use source .venv/bin/active to activate the environment